<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A step-by-step reasoning framework for 3D scene understanding.">
  <meta name="keywords" content="3D Scene Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SceneCOT: Eliciting Chain-of-Thought Reasoning in 3D Scenes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
  <script type="importmap">
    {
      "imports": {
        "vue": "https://unpkg.com/vue@3/dist/vue.esm-browser.js",
        "three": "https://unpkg.com/three@0.127.0/build/three.module.js",
        "three/addons/": "https://unpkg.com/three@0.127.0/examples/jsm/"
      }
    }
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://msr3d.github.io/">
            MSR3D
          </a>
          <a class="navbar-item" href="https://beacon-3d.github.io/">
            Beacon3D
          </a>
          <a class="navbar-item" href="https://sqa3d.github.io/">
            SQA3D
          </a>
          <a class="navbar-item" href="https://embodied-generalist.github.io/">
            LEO
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<style>
  .scene {
    font-family: "Times New Roman", serif;
    font-weight: bold;
    font-size: 36px;
    text-transform: uppercase;
  }

  .cot {
    font-family: "Times New Roman", serif;
    font-weight: normal;
    font-size: 36px;
    text-transform: uppercase;
  }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <!-- <span style="font-variant: small-caps;">SceneCOT</span>: -->
            <span class="title is-1 publication-title">SceneCOT: Eliciting Chain-of-Thought Reasoning in 3D Scenes</span>
          </h1>
          <!-- <h1 class="title is-4 publication-title">NeurIPS 2024 Datasets and Benchmarks</h1> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://xiongkunlinghu.github.io/" rel="external nofollow noopener" target="_blank">Xiongkun Linghu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://huangjy-pku.github.io/" rel="external nofollow noopener" target="_blank">Jiangyong Huang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Ziyu_Zhu1" rel="external nofollow noopener" target="_blank">Ziyu Zhu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://buzz-beater.github.io/" rel="external nofollow noopener" target="_blank">Baoxiong Jia</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://siyuanhuang.com/" rel="external nofollow noopener" target="_blank">Siyuan Huang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing Institute for General Artificial Intelligence,</span>
            <span class="author-block"><sup>2</sup>Peking University</span>
          </div>
        
          <div class="is-size-8 publication-authors">
            <span class="author-block">* indicates equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/SceneCOT/scenecot" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/SceneCOT/scenecot" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/SceneCOT/scenecot" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing research on 3D Large Language Models (LLMs) still struggles to achieve efficient and explainable reasoning, primarily due to the under-exploration of <i>the mechanism of human-like scene-object grounded reasoning</i>. This paper bridges the gap by presenting a novel framework.
          </p>
          <p>
            We first introduce a Chain-of-Thought reasoning method in 3D scenes (<b>SceneCOT</b>), decoupling a complex reasoning task into simpler and manageable problems, and building corresponding visual clues based on multimodal expert modules. To enable such a method, we build the first large-scale 3D scene Chain-of-Thought reasoning dataset <b>SceneCOT-3D</b>, including <b>200K+</b> high-quality data instances.
          </p>
          <p>
            Extensive experiments across various complex 3D scene reasoning benchmarks demonstrate that our new framework achieves state-of-the-art with clear interpretability. To our knowledge, this is the first attempt to successfully implement the COT technique for achieving human-like step-by-step reasoning for 3D scene understanding, where we show great potential in extending it to a wider range of 3D scene understanding scenarios.
          </p>          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Example of Reasoning Chain</h2>
    
        <div class="content has-text-justified">
            <img src="./static/images/teaser_v3_crop.jpg" 
                 alt="Reasoning chain visualization of SceneCOT" 
                 style="display: block; margin: 0 auto 2rem auto; width: 80%; height: auto;">
            <p>
              <strong>Reasoning chain visualization of SceneCOT.</strong> The reasoning chain mimics humans' recognition process: from high-level task recognition to low-level visual semantic understanding.
            </p>
        </div>
      </div>
    </div>    

    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">SceneCOT framework</h2>
    
        <div class="content has-text-justified">
          <img src="./static/images/model_framework_v3_crop.jpg" 
           alt="Detailed overview of benchmarking tasks" 
           style="display: block; margin: 0 auto 2rem auto; width: 80%; height: auto;">
          <p>
            <strong>SceneCOT framework.</strong> Our framework enables interpretable step-by-step reasoning in 3D scenes through a Chain-of-Thought (CoT) process. The reasoning unfolds in four key stages:
          </p>
          <ol>
            <li>
              <strong>Task Recognition:</strong> We identify the task type (e.g., attribute) and provide an initial plan using a <code>&lt;think_type&gt;</code> token.
            </li>
            <li>
              <strong>Region Localization:</strong> We localize the relevant subregion in the 3D scene using directional or clock-based hints. This is marked with <code>&lt;ground_rgn&gt;</code> and <code>&lt;think_rgn&gt;</code>.
            </li>
            <li>
              <strong>Entity Grounding:</strong> Target objects are grounded using semantic and relational cues. The <code>[OBJ]</code> token triggers specialized grounding modules.
            </li>
            <li>
              <strong>Grounded Reasoning:</strong> Based on the grounded entities, we guide the model to perform task-specific operations using <code>&lt;think_task&gt;</code>. The model may access:
              <ul>
                <li><em>Object Probabilities</em> (<code>&lt;obj_prob&gt;</code>)</li>
                <li><em>3D Object Locations</em> (<code>&lt;obj_loc_prob&gt;</code>, <code>&lt;obj_loc_plr_prob&gt;</code>)</li>
                <li><em>Visual Tokens</em> (<code>&lt;highlight_obj&gt;</code>, <code>&lt;img&gt;</code>) for appearance-based reasoning</li>
              </ul>
            </li>
          </ol>
          <p>
            The reasoning ends with a summarization step (<code>&lt;thin_sum&gt;</code>) followed by the final answer (<code>&lt;answer&gt;</code>).
          </p>

        </div>
      </div>
    </div>
    

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>

<script type="module">
  import { createApp } from 'vue'

  function get_layout_config() {
    let w = document.documentElement.clientWidth;
    // console.log(w)
    let c
    let o
    if (w >= 1200) {
      c = 'is-6'
      o = 'is-offset-3'
    } else if (w >= 900) {
      c = 'is-8'
      o = 'is-offset-2'
    }

    else {
        c = 'is-10'
        o = 'is-offset-1'
    }
    return {'content': c, 'offset': o}
  }

  let content = document.getElementById('main-content')
  // content.classList.remove('is-6')
  // content.classList.remove('is-offset-3')

  let app = createApp({
    data() {
      return get_layout_config()
    }
  }).mount('#app')

  // function displayWindowSize() {
  //   let layout = get_layout_config()
  //   app.$data.content = layout['content']
  //   app.$data.offset = layout['offset']
  // }
  // window.addEventListener("resize", displayWindowSize)
  // displayWindowSize()

  import * as THREE from 'three'
  import { OrbitControls } from 'three/addons/controls/OrbitControls.js'
  import {GLTFLoader} from 'three/addons/loaders/GLTFLoader.js'
  // import sqa3d_data from 'https://sqascannetviz.s3.us-west-1.amazonaws.com/sqa3d_website/example_sqa3d_data.json' assert {type: 'json'};
  import sqa3d_data from './assets/example_sqa3d_data.json' assert {type: 'json'};

  // scene & situation visualization
  let canvas2 = document.querySelector('#webgl_scene')
  let scene2 = new THREE.Scene()
  let assetLoader2 = new GLTFLoader()
  let model2

  let camera2 = new THREE.PerspectiveCamera(45, 1.618 / 1.0, 0.1, 100)
  camera2.position.set(5, 4, -5)
  camera2.add(new THREE.PointLight(0xffffff, 0.5))
  scene2.add(camera2)
  for (let ax = 0; ax < 3; ax++) {
    for (let i = 0; i <= 1; i++) {
      let spotLight = new THREE.SpotLight(0xAAAAAA)
      spotLight.position.set(
        ax == 0 ? 50 * (i * 2 - 1): 0,
        ax == 1 ? 50 * (i * 2 - 1): 0,
        ax == 2 ? 50 * (i * 2 - 1): 0,
      )
      scene2.add(spotLight)
    }
  }
  scene2.add(new THREE.HemisphereLight(0xffffff, 0xffffff, 0.6))
  let controls2 = new OrbitControls(camera2, canvas2)
  controls2.enableZoom = true
  // controls2.enableDamping = true
  controls2.object.position.set(camera2.position.x, camera2.position.y, camera2.position.z)
  controls2.target = new THREE.Vector3(0, 0, 0)
  controls2.update()

  let renderer2 = new THREE.WebGLRenderer({
      canvas: canvas2,
      alpha: true,
  })
  renderer2.setPixelRatio(Math.min(window.devicePixelRatio, 2))
  renderer2.outputEncoding = THREE.sRGBEncoding
  renderer2.setAnimationLoop(() => {
    renderer2.render(scene2, camera2)
  });

  const sceneSel = document.querySelector('#sceneSelector')
  const sitSel = document.querySelector('#situationSelector')
  const qSel = document.querySelector('#questionSelector')
  sceneSel.value = 'scene0426_00'
  sitSel.value = 'situation_0'

  // function load_model2(e) {
  //   if (sceneSel.value == '' || sitSel.value == '') {
  //     return
  //   }
  //   scene2.remove(model2)
  //   // document.querySelector('#loading').innerHTML = `<img src="https://sqascannetviz.s3.us-west-1.amazonaws.com/sqa3d_website/icons/loading.svg" width="48" height="48">`
  //   // let assetUrl = new URL(`https://sqascannetviz.s3.us-west-1.amazonaws.com/sqa3d_website/${sceneSel.value + "_00"}/${sitSel.value}.glb`, import.meta.url)
  //   document.querySelector('#loading').innerHTML = `<img src="assets/icons/loading.svg" width="48" height="48">`;
  //   let assetUrl = `assets/${sceneSel.value + "_00"}/${sitSel.value}.glb`;
  //   let scene_id = sceneSel.value + "_00"
  //   let situation = sqa3d_data[scene_id][sitSel.value][0]
  //   let questions = sqa3d_data[scene_id][sitSel.value][1]
  //   document.querySelector("#sit_text").innerHTML = `<span style="color: #ffffff;background-color: #00dd00">Situation</span> ${situation}</span>`
  //   document.querySelector("#q_text").innerHTML = ''
  //   for (var i = 0; i < questions.length; i++) {
  //     document.querySelector("#q_text").innerHTML += `<span style="color: #ffffff;background-color: #ffa500">Question ${i}</span> ${questions[i]}</span><br>`
  //   }

  //   // for (var item in example_sqa3d_data) {
  //   //   let situation_id = "situation_" + qid2situation[item]["number"]
  //   //   let scene_id = sceneSel.value + "_00"
  //   //   if (qid2situation[item]["scene"] == scene_id && situation_id == sitSel.value) {
  //   //     document.querySelector("#sit_text").innerHTML = `<span style="color: #ffffff;background-color: #00dd00">Situation</span> ${qid2situation[item]["situation"]}</span>`
  //   //     break
  //   //   }
  //   // }
  //   assetLoader2.load(assetUrl.href, glb => {
  //     model2 = glb.scene
  //     scene2.add(model2)
  //     document.querySelector('#loading').innerHTML = ''
  //   }, undefined, (error) => {console.error(error)})
  // }

//   function load_model2(e) {
//     if (sceneSel.value == '' || sitSel.value == '') {
//         return;
//     }
//     scene2.remove(model2);
//     document.querySelector('#loading').innerHTML = `<img src="assets/icons/loading.svg" width="48" height="48">`;

//     // Correctly assign assetUrl as a string
//     let assetUrl = `assets/${sceneSel.value + "_00"}/${sitSel.value}.glb`;

//     let scene_id = sceneSel.value + "_00";
//     console.log(sqa3d_data)
//     let situation = sqa3d_data[scene_id][sitSel.value][0];
//     let questions = sqa3d_data[scene_id][sitSel.value][1];

//     document.querySelector("#sit_text").innerHTML = `<span style="color: #ffffff;background-color: #00dd00">Situation</span> ${situation}`;
//     document.querySelector("#q_text").innerHTML = '';

//     for (var i = 0; i < questions.length; i++) {
//         document.querySelector("#q_text").innerHTML += `<span style="color: #ffffff;background-color: #ffa500">Question ${i}</span> ${questions[i]}<br>`;
//     }

//     assetLoader2.load(assetUrl, glb => {
//         model2 = glb.scene;
//         scene2.add(model2);
//         document.querySelector('#loading').innerHTML = '';
//     }, undefined, error => {
//         console.error("Failed to load GLB file:", error);
//     });
// }


//   sceneSel.addEventListener('change', (e) => {sitSel.value = ''})
//   sitSel.addEventListener('change', load_model2)
//   load_model2()

//   // resize renderers
//   function resizeRenderers() {
//     let content_width = document.querySelector('#results').offsetWidth
//     renderer2.setSize(content_width, content_width / 1.618)
//   }
//   window.addEventListener('resize', () => {
//     resizeRenderers()
//   })
//   resizeRenderers()
// Define a variable to hold the mapping dictionary
// Define a variable to hold the mapping dictionary
  let imageMapping = {};

  // Fetch and store the image mapping dictionary
  async function fetchImageMapping() {
      try {
          const response = await fetch('/assets/mapping_dict.json');
          imageMapping = await response.json();
          console.log('Image mapping loaded:', imageMapping);
      } catch (error) {
          console.error('Failed to load image mapping:', error);
      }
  }

    // Function to replace placeholders with actual image tags
  function insertImages(text, sceneId) {
      const sceneImages = imageMapping[sceneId] || {};
      return text.replace(/<\w+-(\d+)-IMG>/g, (match, key) => {
          // Extracts the numeric key
          const imageUrl = sceneImages[key];
          console.log('Match:', match);
          console.log('Key:', key);
          console.log('Scene Images:', sceneImages);
          console.log('Image URL:', imageUrl);
          if (imageUrl) {
              return `<img src="${imageUrl}" alt="image ${key}" style="width:100px;">`; // Adjust width as necessary
          }
          return match; // No image found, return the placeholder
      });
  }

  // Function to load the model and update the situation and question text
  async function load_model2() {
      let sceneId = sceneSel.value;
      let sitId = sitSel.value;

      if (!sceneId || !sitId) {
          console.log('Either scene ID or situation ID is missing.');
          return;
      }

      // Fetch new model data
      await fetchModelData(sceneId, sitId);
      updateSceneDisplay(); // Update the scene and texts
  }

  // Fetch model data based on scene and situation IDs
  async function fetchModelData(sceneId, sitId) {
      document.querySelector('#loading').innerHTML = `<img src="assets/icons/loading.svg" width="48" height="48">`;
      let assetUrl = `assets/${sceneId}/${sitId}.glb`;

      try {
          let glb = await assetLoader2.loadAsync(assetUrl);
          scene2.remove(model2); // Remove previous model
          model2 = glb.scene;
          scene2.add(model2);
          document.querySelector('#loading').innerHTML = '';
      } catch (error) {
          console.error("Failed to load GLB file:", error);
          document.querySelector('#loading').innerHTML = 'Failed to load scene.';
      }
  }

  // Update the display of the current situation and questions
  function updateSceneDisplay() {
      let scene_part_id = sceneSel.value;
      let sceneId = `${scene_part_id}`;
      let sitId = sitSel.value;
      console.log('scene id:', sceneId)
      console.log('data scene sit id:', sqa3d_data[sceneId])
      let situationText = sqa3d_data[sceneId][sitId][0];
      let questions = sqa3d_data[sceneId][sitId][1];

      // Insert images and update HTML
      document.querySelector("#sit_text").innerHTML = `<span style="color: #ffffff;background-color: #00dd00">Situation:</span> ${insertImages(situationText, sceneId)}`;
      document.querySelector("#q_text").innerHTML = questions.map((q, i) => `<span style="color: #ffffff;background-color: #ffa500">Question ${i}:</span> ${insertImages(q, sceneId)}`).join('<br>');
  }

  sceneSel.addEventListener('change', load_model2);
  sitSel.addEventListener('change', load_model2);

  // Initialize on page load
  document.addEventListener('DOMContentLoaded', async function () {
      await fetchImageMapping();
      load_model2(); // Load initial model and text
  });

  // Ensure the renderer resizes correctly
  function resizeRenderers() {
      let content_width = document.querySelector('#results').offsetWidth;
      renderer2.setSize(content_width, content_width / 1.618);
  }
  window.addEventListener('resize', resizeRenderers);
  resizeRenderers();


</script>

</html>
